+++
title = "Prompt工程入门"
author = ["LAPTOP-D0OQP29H"]
lastmod = 2024-06-27T21:49:29+08:00
tags = ["AI", "Prompt"]
categories = ["Tech"]
draft = false
toc = true
+++

本文主要内容主要来自于吴恩达的《ChatGPT Prompt Engineering for Developers》，并省略了其中的代码内容


## 概念补全 {#概念补全}

-   LLM：
    -   Base LLM：通过大量训练数据，来预测下一句话
    -   Instruction Tune LLM：被训练的可以遵循指令
        -   RLHF：从人类反馈中学习


## 提供精确的指令 {#提供精确的指令}

很多时候模型不能正常工作，或者不和预期，是因为指令不够精确导致的。
清晰并不等价于短，有时候长的指令会更加清晰明确

1.  使用分界符区分指令与数据，并对各个部分进行标注分割，例如“总结反引号包含的内容”，同时这也可以防止指令注入
2.  要求结构化的输出，例如“使用包含如下Key的JSON返回，key1，key2”，清晰美观，程序可以直接处理
3.  使用先验知识，让模型进行前置的条件检查，而不是让他瞎编。例如“如果输入内容不包含xxx，返回xxx”
4.  向模型提供一些正确处理的例子，并要求的执行同样的任务。例如“进行同样风格的回答：xxxxx”


## 让模型有时间思考 {#让模型有时间思考}

算力问题，我们不希望模型着急回答，而是引导其思考。
本质上来说是因为模型没有办法像人一样快速地推导出求解流程，需要通过提示词工程，将人类思考的逻辑注入进来，来加快求解。

1.  明确执行步骤，例如“步骤1xxx；步骤2xxxx”
2.  要求模型先自行解答，例如对一个问题，判定答案是否正确。这一条更像是判断类型下的一个执行步骤优化。


## 模型的局限性 {#模型的局限性}

尽管模型接触到了大量的训练数据，但是他并不清楚自己知识的边界是什么。也就是瞎编内容，称Hallucination幻觉。
一个从输入预测输出的机器，可以总是能输出一些内容，哪怕这些内容都是瞎编的。
解决这个问题的思路：首先找到相关信息，并引用他们来回答问题，追溯到参考文献后幻觉少了很多。


## 如何不断迭代提示词 {#如何不断迭代提示词}

一次写出要给完美的提示词是不现实的，我们必须学会如何不断地迭代优化现有提示，并最终达成目标。
如何迭代：Idea-&gt;Implementation-&gt;Experimental Result-&gt;Error Analysis-&gt;Idea
如何进行Error Analysis：这里并没有一大套方法论，更像是一个阅读题。你阅读模型的返回，像一个无耻的甲方一样提出修改意见。当然，后面修改的乙方也是你。


## 四项常用的能力 {#四项常用的能力}

1.  总结
2.  推理
3.  转换
4.  扩展


## Temperature {#temperature}

一个模型的参数，会影响相应的随机性或者说多样性
例如输入我最爱吃的食物，Temperature为0是会选择输出评分最高的答案披萨。
我们提升Temperature后，排序靠后的一些答案也会被输出，这个参数解决答案的“饥饿”问题。
构建可靠可预测的系统时，这个参数应被设置为0


## 多轮对话的构成 {#多轮对话的构成}

我们每一次提交的message包含如下几个部分

```js
{
        "role": "user",
        "content": "你好"
}
```
其中role有三种，分别为system，assistant，user。
user代表用户的输入内容，assistant是模型的输出内容，system用于对assistant进行声明设定，是层次更高程的meta。
那么一段对话可以表示为：
```
[
 {
        "role": "system",
        "content": "你是一个逗逼"
  },
  {
        "role": "user",
        "content": "给我讲个笑话"
  },
  {
        "role": "assistant",
        "content": "从前....."
  }
]
```

这一串内容也就是上下文的大小，也就是模型里我们看到的1k后缀。
